{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading latest crypto data from Yahoo Finance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['ADA-USD']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved ADA data to C:\\Crypto\\datareq\\ADA.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['BNB-USD']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved BNB data to C:\\Crypto\\datareq\\BNB.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['BTC-USD']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved BTC data to C:\\Crypto\\datareq\\BTC.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['DOGE-USD']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved DOGE data to C:\\Crypto\\datareq\\DOGE.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get ticker 'ETH-USD' reason: Expecting value: line 1 column 1 (char 0)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['ETH-USD']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved ETH data to C:\\Crypto\\datareq\\ETH.csv\n",
      "\n",
      "üìä Processing ADA...\n",
      "\n",
      "‚ùå Error processing ADA: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by MinMaxScaler.\n",
      "\n",
      "üìä Processing BNB...\n",
      "\n",
      "‚ùå Error processing BNB: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by MinMaxScaler.\n",
      "\n",
      "üìä Processing BTC...\n",
      "\n",
      "‚ùå Error processing BTC: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by MinMaxScaler.\n",
      "\n",
      "üìä Processing DOGE...\n",
      "\n",
      "‚ùå Error processing DOGE: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by MinMaxScaler.\n",
      "\n",
      "üìä Processing ETH...\n",
      "\n",
      "‚ùå Error processing ETH: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by MinMaxScaler.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import yfinance as yf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import r2_score\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "# from datetime import timedelta\n",
    "\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# crypto_symbols = ['ADA-USD', 'BTC-USD', 'BNB-USD', 'DOGE-USD','ETC-USD']  \n",
    "# start_date = '2020-01-01'\n",
    "# end_date = '2025-08-11'\n",
    "\n",
    "# folder_path = r'C:\\CRYPTO\\datareq'\n",
    "# os.makedirs(folder_path, exist_ok=True) \n",
    "\n",
    "# def download_crypto_data():\n",
    "#     for crypto_symbol in crypto_symbols:\n",
    "#         print(f\"Downloading data for {crypto_symbol}...\")\n",
    "#         try:\n",
    "#             crypto_data = yf.download(crypto_symbol, start=start_date, end=end_date)\n",
    "#             crypto_data.reset_index(inplace=True)\n",
    "#             csv_file_path = os.path.join(folder_path, f'{crypto_symbol}.csv')\n",
    "#             crypto_data.to_csv(csv_file_path, index=False)\n",
    "#             print(f'Data for {crypto_symbol} saved to {csv_file_path}')\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error downloading {crypto_symbol}: {e}\")\n",
    "\n",
    "# def process_and_train():\n",
    "#     for file_name in os.listdir(folder_path):\n",
    "#         if file_name.endswith('.csv'):\n",
    "#             file_path = os.path.join(folder_path, file_name)\n",
    "#             print(f\"\\nProcessing {file_name}...\\n\")\n",
    "#             try:\n",
    "#                 ADA = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "#                 print(ADA.head())\n",
    "#                 print(ADA.dtypes)\n",
    "\n",
    "#                 if 'Date' in ADA.columns:\n",
    "#                     ADA['date'] = pd.to_datetime(ADA['Date'], errors='coerce')\n",
    "#                 else:\n",
    "#                     raise ValueError(f\"'Date' column not found in {file_name}\")\n",
    "\n",
    "#                 necessary_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "#                 for col in necessary_columns:\n",
    "#                     ADA[col] = pd.to_numeric(ADA[col], errors='coerce')# convert to numeric values\n",
    "\n",
    "#                 ADA = ADA.dropna(subset=['Close', 'Open', 'High', 'Low', 'Volume']) #drop null values\n",
    "\n",
    "#                 ADA['MA5'] = ADA['Close'].rolling(window=5).mean()\n",
    "#                 ADA['MA10'] = ADA['Close'].rolling(window=10).mean()\n",
    "#                 ADA['MA20'] = ADA['Close'].rolling(window=20).mean()\n",
    "\n",
    "#                 ADA['Next_Close'] = ADA['Close'].shift(-1)\n",
    "#                 ADA = ADA.dropna() \n",
    "\n",
    "#                 ADA_features = ADA[['Open', 'Low', 'High', 'Close', 'MA5', 'MA10', 'MA20']]\n",
    "#                 ADA_target = ADA['Next_Close']\n",
    "\n",
    "#                 ADA_scaler = MinMaxScaler()\n",
    "#                 ADA_features_scaled = ADA_scaler.fit_transform(ADA_features)\n",
    "\n",
    "#                 ADA_target_scaler = MinMaxScaler()\n",
    "#                 ADA_target_scaled = ADA_target_scaler.fit_transform(ADA_target.values.reshape(-1, 1))\n",
    "\n",
    "#                 def create_sequences(features, target, n_steps):\n",
    "#                     X, y = [], []\n",
    "#                     for i in range(len(features) - n_steps):\n",
    "#                         X.append(features[i:i + n_steps])\n",
    "#                         y.append(target[i + n_steps])\n",
    "#                     return np.array(X), np.array(y)\n",
    "\n",
    "#                 n_steps = 30\n",
    "#                 X_ADA, y_ADA = create_sequences(ADA_features_scaled, ADA_target_scaled, n_steps)\n",
    "#                 X_train_ADA, X_test_ADA, y_train_ADA, y_test_ADA = train_test_split(X_ADA, y_ADA, test_size=0.2, random_state=42)\n",
    "\n",
    "#                 ADA_LSTM_model = Sequential([\n",
    "#                     LSTM(units=128, return_sequences=True, input_shape=(X_train_ADA.shape[1], X_train_ADA.shape[2])),\n",
    "#                     Dropout(0.1),\n",
    "#                     LSTM(units=64),\n",
    "#                     Dropout(0.1),\n",
    "#                     Dense(units=32, activation='relu'),\n",
    "#                     Dense(units=1)\n",
    "#                 ])\n",
    "\n",
    "#                 optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#                 ADA_LSTM_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "#                 ADA_LSTM_history = ADA_LSTM_model.fit(X_train_ADA, y_train_ADA, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "#                 y_test_pred_scaled = ADA_LSTM_model.predict(X_test_ADA)\n",
    "\n",
    "#                 y_test_pred = ADA_target_scaler.inverse_transform(y_test_pred_scaled)\n",
    "#                 y_test_actual = ADA_target_scaler.inverse_transform(y_test_ADA)\n",
    "\n",
    "#                 r2 = r2_score(y_test_actual, y_test_pred)\n",
    "#                 print(f\"R¬≤ Score on test data for {file_name}: {r2:.4f}\")\n",
    "\n",
    "#                 end_index = len(ADA_features_scaled) - 1\n",
    "#                 start_index = end_index - n_steps + 1\n",
    "#                 sequence = ADA_features_scaled[start_index:end_index + 1]\n",
    "#                 sequence = np.expand_dims(sequence, axis=0)\n",
    "\n",
    "#                 ADA_predicted_scaled = ADA_LSTM_model.predict(sequence)\n",
    "#                 ADA_predicted_scaled_reshaped = ADA_predicted_scaled.reshape(-1, 1)\n",
    "#                 ADA_predicted = ADA_target_scaler.inverse_transform(ADA_predicted_scaled_reshaped)\n",
    "\n",
    "#                 last_date = ADA['date'].max()\n",
    "#                 next_date = last_date + timedelta(days=1)\n",
    "\n",
    "#                 predicted_price = ADA_predicted[0][0]\n",
    "#                 print(f'Predicted closing price for {next_date.strftime(\"%d-%b-%Y\")} in {file_name}: {predicted_price:.2f}')\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# download_crypto_data()  \n",
    "# process_and_train()\n",
    "\n",
    "\n",
    "# # import os\n",
    "# # import pandas as pd\n",
    "# # import numpy as np\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "# # from sklearn.preprocessing import MinMaxScaler\n",
    "# # from sklearn.metrics import r2_score\n",
    "# # import tensorflow as tf\n",
    "# # from tensorflow.keras.models import Sequential\n",
    "# # from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "# # from datetime import timedelta\n",
    "\n",
    "# # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# # # ‚úÖ Path where your dataset is stored\n",
    "# # folder_path = r'C:\\Crypto\\dataset'\n",
    "# # file_name = 'ada.csv'   # <--- Your dataset file\n",
    "\n",
    "# # def process_and_train():\n",
    "# #     file_path = os.path.join(folder_path, file_name)\n",
    "# #     print(f\"\\nProcessing {file_name}...\\n\")\n",
    "# #     try:\n",
    "# #         ADA = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# #         print(ADA.head())\n",
    "# #         print(ADA.dtypes)\n",
    "\n",
    "# #         # ‚úÖ Convert date column\n",
    "# #         ADA['date'] = pd.to_datetime(ADA['date'], errors='coerce')\n",
    "\n",
    "# #         # ‚úÖ Convert numeric columns\n",
    "# #         numeric_cols = ['open', 'high', 'low', 'close']\n",
    "# #         for col in numeric_cols:\n",
    "# #             ADA[col] = pd.to_numeric(ADA[col], errors='coerce')\n",
    "\n",
    "# #         # ‚úÖ Drop null values\n",
    "# #         ADA = ADA.dropna(subset=['close', 'open', 'high', 'low'])\n",
    "\n",
    "# #         # ‚úÖ Moving averages (based on close price)\n",
    "# #         ADA['MA5'] = ADA['close'].rolling(window=5).mean()\n",
    "# #         ADA['MA10'] = ADA['close'].rolling(window=10).mean()\n",
    "# #         ADA['MA20'] = ADA['close'].rolling(window=20).mean()\n",
    "\n",
    "# #         # ‚úÖ Target column (next day close price)\n",
    "# #         ADA['Next_Close'] = ADA['close'].shift(-1)\n",
    "# #         ADA = ADA.dropna()\n",
    "\n",
    "# #         # ‚úÖ Features (no volume)\n",
    "# #         ADA_features = ADA[['open', 'low', 'high', 'close', 'MA5', 'MA10', 'MA20']]\n",
    "# #         ADA_target = ADA['Next_Close']\n",
    "\n",
    "# #         # ‚úÖ Scaling\n",
    "# #         ADA_scaler = MinMaxScaler()\n",
    "# #         ADA_features_scaled = ADA_scaler.fit_transform(ADA_features)\n",
    "\n",
    "# #         ADA_target_scaler = MinMaxScaler()\n",
    "# #         ADA_target_scaled = ADA_target_scaler.fit_transform(ADA_target.values.reshape(-1, 1))\n",
    "\n",
    "# #         # ‚úÖ Create sequences for LSTM\n",
    "# #         def create_sequences(features, target, n_steps):\n",
    "# #             X, y = [], []\n",
    "# #             for i in range(len(features) - n_steps):\n",
    "# #                 X.append(features[i:i + n_steps])\n",
    "# #                 y.append(target[i + n_steps])\n",
    "# #             return np.array(X), np.array(y)\n",
    "\n",
    "# #         n_steps = 30\n",
    "# #         X_ADA, y_ADA = create_sequences(ADA_features_scaled, ADA_target_scaled, n_steps)\n",
    "# #         X_train_ADA, X_test_ADA, y_train_ADA, y_test_ADA = train_test_split(X_ADA, y_ADA, test_size=0.2, random_state=42)\n",
    "\n",
    "# #         # ‚úÖ LSTM Model\n",
    "# #         ADA_LSTM_model = Sequential([\n",
    "# #             LSTM(units=128, return_sequences=True, input_shape=(X_train_ADA.shape[1], X_train_ADA.shape[2])),\n",
    "# #             Dropout(0.1),\n",
    "# #             LSTM(units=64),\n",
    "# #             Dropout(0.1),\n",
    "# #             Dense(units=32, activation='relu'),\n",
    "# #             Dense(units=1)\n",
    "# #         ])\n",
    "\n",
    "# #         optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# #         ADA_LSTM_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# #         # ‚úÖ Train model\n",
    "# #         ADA_LSTM_model.fit(\n",
    "# #             X_train_ADA, y_train_ADA,\n",
    "# #             epochs=50, batch_size=64, validation_split=0.2,\n",
    "# #             verbose=1\n",
    "# #         )\n",
    "\n",
    "# #         # ‚úÖ Predictions\n",
    "# #         y_test_pred_scaled = ADA_LSTM_model.predict(X_test_ADA)\n",
    "# #         y_test_pred = ADA_target_scaler.inverse_transform(y_test_pred_scaled)\n",
    "# #         y_test_actual = ADA_target_scaler.inverse_transform(y_test_ADA)\n",
    "\n",
    "# #         r2 = r2_score(y_test_actual, y_test_pred)\n",
    "# #         print(f\"R¬≤ Score on test data for {file_name}: {r2:.4f}\")\n",
    "\n",
    "# #         # ‚úÖ Predict next day\n",
    "# #         end_index = len(ADA_features_scaled) - 1\n",
    "# #         start_index = end_index - n_steps + 1\n",
    "# #         sequence = ADA_features_scaled[start_index:end_index + 1]\n",
    "# #         sequence = np.expand_dims(sequence, axis=0)\n",
    "\n",
    "# #         ADA_predicted_scaled = ADA_LSTM_model.predict(sequence)\n",
    "# #         ADA_predicted = ADA_target_scaler.inverse_transform(ADA_predicted_scaled)\n",
    "\n",
    "# #         last_date = ADA['date'].max()\n",
    "# #         next_date = last_date + timedelta(days=1)\n",
    "\n",
    "# #         predicted_price = ADA_predicted[0][0]\n",
    "# #         print(f'Predicted closing price for {next_date.strftime(\"%d-%b-%Y\")} in {file_name}: {predicted_price:.2f}')\n",
    "\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# # # ‚úÖ Run training directly\n",
    "# # process_and_train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import r2_score\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "# from datetime import timedelta\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# # ‚úÖ Path where your dataset is stored\n",
    "# folder_path = r'C:\\Crypto\\dataset'\n",
    "\n",
    "# # ‚úÖ List of coins you want to run\n",
    "# crypto_files = {\n",
    "#     \"ADA\": \"ADA.csv\",\n",
    "#     \"BNB\": \"BNB.csv\",\n",
    "#     \"BTC\": \"BTC.csv\",\n",
    "#     \"DOGE\": \"DOGE.csv\",\n",
    "#     \"ETH\": \"ETH.csv\"\n",
    "# }\n",
    "\n",
    "# def process_and_train(symbol, file_name):\n",
    "#     file_path = os.path.join(folder_path, file_name)\n",
    "#     print(f\"\\nüìä Processing {symbol} ({file_name})...\\n\")\n",
    "#     try:\n",
    "#         df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "#         # ‚úÖ Convert date column\n",
    "#         df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "#         ‚úÖ Convert numeric columns\n",
    "#         numeric_cols = ['open', 'high', 'low', 'close']\n",
    "#         for col in numeric_cols:\n",
    "#             df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "#         # ‚úÖ Drop null values\n",
    "#         df = df.dropna(subset=['close', 'open', 'high', 'low'])\n",
    "\n",
    "#         # ‚úÖ Moving averages (based on close price)\n",
    "#         df['MA5'] = df['close'].rolling(window=5).mean()\n",
    "#         df['MA10'] = df['close'].rolling(window=10).mean()\n",
    "#         df['MA20'] = df['close'].rolling(window=20).mean()\n",
    "\n",
    "#         # ‚úÖ Target column (next day close price)\n",
    "#         df['Next_Close'] = df['close'].shift(-1)\n",
    "#         df = df.dropna()\n",
    "\n",
    "#         # ‚úÖ Features (no volume)\n",
    "#         features = df[['open', 'low', 'high', 'close', 'MA5', 'MA10', 'MA20']]\n",
    "#         target = df['Next_Close']\n",
    "\n",
    "#         # ‚úÖ Scaling\n",
    "#         scaler_features = MinMaxScaler()\n",
    "#         features_scaled = scaler_features.fit_transform(features)\n",
    "\n",
    "#         scaler_target = MinMaxScaler()\n",
    "#         target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "#         # ‚úÖ Create sequences for LSTM\n",
    "#         def create_sequences(features, target, n_steps):\n",
    "#             X, y = [], []\n",
    "#             for i in range(len(features) - n_steps):\n",
    "#                 X.append(features[i:i + n_steps])\n",
    "#                 y.append(target[i + n_steps])\n",
    "#             return np.array(X), np.array(y)\n",
    "\n",
    "#         n_steps = 30\n",
    "#         X, y = create_sequences(features_scaled, target_scaled, n_steps)\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#         # ‚úÖ LSTM Model\n",
    "#         model = Sequential([\n",
    "#             LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "#             Dropout(0.1),\n",
    "#             LSTM(units=64),\n",
    "#             Dropout(0.1),\n",
    "#             Dense(units=32, activation='relu'),\n",
    "#             Dense(units=1)\n",
    "#         ])\n",
    "\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#         model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "#         # ‚úÖ Train model\n",
    "#         model.fit(\n",
    "#             X_train, y_train,\n",
    "#             epochs=50, batch_size=64, validation_split=0.2,\n",
    "#             verbose=0\n",
    "#         )\n",
    "\n",
    "#         # ‚úÖ Predictions\n",
    "#         y_test_pred_scaled = model.predict(X_test, verbose=0)\n",
    "#         y_test_pred = scaler_target.inverse_transform(y_test_pred_scaled)\n",
    "#         y_test_actual = scaler_target.inverse_transform(y_test)\n",
    "\n",
    "#         r2 = r2_score(y_test_actual, y_test_pred)\n",
    "#         print(f\"‚úÖ R¬≤ Score on test data for {symbol}: {r2:.4f}\")\n",
    "\n",
    "#         # ‚úÖ Predict next day\n",
    "#         end_index = len(features_scaled) - 1\n",
    "#         start_index = end_index - n_steps + 1\n",
    "#         sequence = features_scaled[start_index:end_index + 1]\n",
    "#         sequence = np.expand_dims(sequence, axis=0)\n",
    "\n",
    "#         predicted_scaled = model.predict(sequence, verbose=0)\n",
    "#         predicted = scaler_target.inverse_transform(predicted_scaled)\n",
    "\n",
    "#         last_date = df['date'].max()\n",
    "#         next_date = last_date + timedelta(days=1)\n",
    "\n",
    "#         predicted_price = predicted[0][0]\n",
    "#         print(f\"üìÖ Predicted closing price for {symbol} on {next_date.strftime('%d-%b-%Y')}: {predicted_price:.2f}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error processing {symbol}: {e}\")\n",
    "\n",
    "# # ‚úÖ Run training for all cryptos\n",
    "# for symbol, file_name in crypto_files.items():\n",
    "#     process_and_train(symbol, file_name)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from datetime import timedelta\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# ‚úÖ Folder to store downloaded data\n",
    "folder_path = r\"C:\\Crypto\\datareq\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# ‚úÖ List of cryptos (Yahoo Finance tickers)\n",
    "crypto_symbols = {\n",
    "    \"ADA\": \"ADA-USD\",\n",
    "    \"BNB\": \"BNB-USD\",\n",
    "    \"BTC\": \"BTC-USD\",\n",
    "    \"DOGE\": \"DOGE-USD\",\n",
    "    \"ETH\": \"ETH-USD\"\n",
    "}\n",
    "\n",
    "# ‚úÖ Download latest crypto data and save as CSV\n",
    "def download_data():\n",
    "    print(\"üì• Downloading latest crypto data from Yahoo Finance...\")\n",
    "    for symbol, ticker in crypto_symbols.items():\n",
    "        try:\n",
    "            df = yf.download(ticker, start=\"2020-01-01\")\n",
    "            df.reset_index(inplace=True)\n",
    "\n",
    "            # Rename columns to match our training code\n",
    "            df.rename(columns={\n",
    "                \"Date\": \"date\",\n",
    "                \"Open\": \"open\",\n",
    "                \"High\": \"high\",\n",
    "                \"Low\": \"low\",\n",
    "                \"Close\": \"close\",\n",
    "            }, inplace=True)\n",
    "\n",
    "            csv_path = os.path.join(folder_path, f\"{symbol}.csv\")\n",
    "            df[['date', 'open', 'high', 'low', 'close']].to_csv(csv_path, index=False)\n",
    "            print(f\"‚úÖ Saved {symbol} data to {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error downloading {symbol}: {e}\")\n",
    "\n",
    "def process_and_train(symbol):\n",
    "    file_path = os.path.join(folder_path, f\"{symbol}.csv\")\n",
    "    print(f\"\\nüìä Processing {symbol}...\\n\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "        # Convert date column\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "        # Convert numeric columns\n",
    "        numeric_cols = [\"open\", \"high\", \"low\", \"close\"]\n",
    "        for col in numeric_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Drop nulls\n",
    "        df = df.dropna(subset=[\"close\", \"open\", \"high\", \"low\"])\n",
    "\n",
    "        # Moving averages\n",
    "        df[\"MA5\"] = df[\"close\"].rolling(window=5).mean()\n",
    "        df[\"MA10\"] = df[\"close\"].rolling(window=10).mean()\n",
    "        df[\"MA20\"] = df[\"close\"].rolling(window=20).mean()\n",
    "\n",
    "        # Next day target\n",
    "        df[\"Next_Close\"] = df[\"close\"].shift(-1)\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Features\n",
    "        features = df[[\"open\", \"low\", \"high\", \"close\", \"MA5\", \"MA10\", \"MA20\"]]\n",
    "        target = df[\"Next_Close\"]\n",
    "\n",
    "        # Scaling\n",
    "        scaler_features = MinMaxScaler()\n",
    "        features_scaled = scaler_features.fit_transform(features)\n",
    "\n",
    "        scaler_target = MinMaxScaler()\n",
    "        target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "        # Create sequences\n",
    "        def create_sequences(features, target, n_steps):\n",
    "            X, y = [], []\n",
    "            for i in range(len(features) - n_steps):\n",
    "                X.append(features[i:i + n_steps])\n",
    "                y.append(target[i + n_steps])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        n_steps = 30\n",
    "        X, y = create_sequences(features_scaled, target_scaled, n_steps)\n",
    "\n",
    "        if len(X) == 0:\n",
    "            print(f\"‚ùå Not enough data for {symbol}. Skipping...\")\n",
    "            return\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # LSTM model\n",
    "        model = Sequential([\n",
    "            LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dropout(0.1),\n",
    "            LSTM(units=64),\n",
    "            Dropout(0.1),\n",
    "            Dense(units=32, activation=\"relu\"),\n",
    "            Dense(units=1)\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mean_squared_error\")\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "        y_pred = scaler_target.inverse_transform(y_pred_scaled)\n",
    "        y_actual = scaler_target.inverse_transform(y_test)\n",
    "\n",
    "        r2 = r2_score(y_actual, y_pred)\n",
    "        print(f\"‚úÖ R¬≤ Score on test data for {symbol}: {r2:.4f}\")\n",
    "\n",
    "        # Predict next day\n",
    "        sequence = features_scaled[-n_steps:]\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "\n",
    "        predicted_scaled = model.predict(sequence, verbose=0)\n",
    "        predicted = scaler_target.inverse_transform(predicted_scaled)\n",
    "\n",
    "        last_date = df[\"date\"].max()\n",
    "        next_date = last_date + timedelta(days=1)\n",
    "\n",
    "        print(f\"üìÖ Predicted closing price for {symbol} on {next_date.strftime('%d-%b-%Y')}: {predicted[0][0]:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {symbol}: {e}\")\n",
    "\n",
    "# ‚úÖ Main Run\n",
    "download_data()\n",
    "for symbol in crypto_symbols.keys():\n",
    "    process_and_train(symbol)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
